#!/usr/bin/env python3
# coordinator.py - V2.1.1: Python Service Orchestrator with Updated Docs
#
# Version: 2.1.1
# Date: 2025-06-24
# Author: Diagnostic Systems Group
#
# Philosophy for this version:
#   - This script orchestrates a DEBUGGING TOOL. The input Markdown is EXPECTED to have errors
#     preventing successful compilation to a final format (e.g., PDF).
#   - The tool's "success" is the generation of a useful and actionable diagnostic report.
#   - If a service component *of the tool itself* (e.g., InitialProcessor, Compiler, Investigator,
#     Oracle, Reporter) crashes or violates its JSON API contract (e.g., missing expected keys,
#     malformed JSON), this Coordinator script will CRASH with a standard Python traceback to stderr.
#     This signals a bug in that service for the tool developer to fix. No internal error handling
#     for such tool/component bugs is performed by this Coordinator.
#   - "User document errors" (e.g., malformed Markdown, LaTeX errors, proofreading issues found
#     in the generated TeX) are *expected inputs* that drive the diagnostic workflow.
#   - The Investigator service is expected to always provide at least one "Lead" if a compilation
#     log is processed (even if it's a generic "compilation failed, review log" lead).
#   - The Oracle service is contractually obligated to ALWAYS provide a `Solution` object for any
#     valid `OracleQuery` it receives. If it cannot find a specific fix, its "solution" must
#     state that and guide the user (e.g., "Manual review required").
#   - Debug logging is controlled by the 'DEBUG' environment variable, via `logger` from `manager_utils.logger_utils`.
#
# ────────────────────────────────────────────────────────────────────────────────
# Role: THE COORDINATOR (Python Service Orchestrator)
#
# This script orchestrates the Markdown → Diagnostic Report pipeline by delegating
# tasks to specialized, independent services. It communicates with these services
# via JSON APIs. It does not perform any log parsing or analysis itself beyond what
# is reported by the services it calls.
#
# The coordinator’s sole responsibility is to:
#   • Execute calls to services in a defined pipeline order (Stages 1-4).
#   • Manage control flow based on JSON responses from services (primarily `status` fields).
#   • Construct appropriate JSON request payloads for each service using data from previous stages.
#   • Ensure a final diagnostic report is generated by the Reporter service for any user document error.
#
# ────────────────────────────────────────────────────────────────────────────────
# Guidelines:
#
#   • Services (implementing Roles) should have a single, well-defined responsibility and a clear JSON API.
#   • This Coordinator script remains thin, focused on sequencing service calls
#     and managing the high-level data flow within the `master_case_file`.
#
# ────────────────────────────────────────────────────────────────────────────────
# Component Service Pipeline & Workflow Stages:
#
#   STAGE 1: Markdown Analysis (`MarkdownLinterAndTexConverter` Service)
#     - Converts input Markdown to LaTeX.
#     - Runs initial static ("proofreading") checks on the generated LaTeX.
#     - Reports: LaTeX content, conversion status, and any `Finding`s.
#     - Outcome determines if pipeline proceeds to TeX compilation or directly to solution generation.
#
#   STAGE 2: TeX Deep Analysis (`LatexCompiler` and `LogInvestigator` Services)
#     - (If Stage 1 allows) Attempts TeX compilation primarily to generate a LaTeX `CompilationLog`.
#     - Analyzes the `CompilationLog` (and source LaTeX) to find specific error `Lead`s.
#
#   STAGE 3: Find Solutions (`OracleSolutionGenerator` Service)
#     - Takes all `Finding`s (from Stage 1) and `Lead`s (from Stage 2).
#     - For each, transforms it into a standardized `OracleQuery`.
#     - Submits each `OracleQuery` to the Oracle service.
#     - Oracle service (internally using Librarian for context) returns a `Solution` for each query.
#
#   STAGE 4: Report to User (`FinalReporter` Service)
#     - Takes all collected `Solution`s and overall status.
#     - Assembles and prints the final human-readable diagnostic report to stdout.
#
# ────────────────────────────────────────────────────────────────────────────────
# Interface Contract (for this coordinator.py)
#
# Invocation (typically by main_entry.py):
#   cat initial_job_payload.json | python3 coordinator.py
#   (initial_job_payload.json MUST contain 'case_id', 'original_document.content',
#    'collected_solutions: []', and 'latex_document_content: null')
#
# Required Environment Variables (set by caller, e.g., main_entry.py):
#   MARKDOWN_LINTER_TEX_CONVERTER_BIN, LATEX_COMPILER_BIN, LOG_INVESTIGATOR_BIN,
#   ORACLE_SOLUTION_GENERATOR_BIN, FINAL_REPORTER_BIN
#   DEBUG (optional, if "true", enables debug output via logger_utils)
#
# Output (to stdout):
#   - The text of the Final Diagnostic Report from the Reporter service.
#
# Exit Codes:
#   - 1: Diagnostics were successfully generated and output by Reporter for a user document error.
#   - Other non-zero (implicit, from Python interpreter on unhandled exception):
#     This script crashed due to a service component failure (non-zero exit, bad JSON output),
#     an internal Python error in this script (e.g., failed assertion), or a setup issue
#     (e.g., missing environment variable for a service BIN, unimportable utils).
#
# Example Diagnostic Output (on stdout, from Reporter service):
#   Error: LaTeX compilation failed
#   Line: 42
#   Log:  ! LaTeX Error: Missing \end{document}.
#   Fix:  Add \end{document} before EOF
#
# ────────────────────────────────────────────────────────────────────────────────
#!/usr/bin/env python3
# coordinator.py - V2.1.1: Explicit sys.path manipulation for manager_utils
#
# Version: 2.1.1
# Date: 2025-06-24
# Author: Diagnostic Systems Group
#
# Philosophy: (Same as V2.1)
# Invocation: (Same as V2.1)
# Output: (Same as V2.1)
# Exit Codes: (Same as V2.1)
# --------------------------------------------------------------------------------
# Key Change in V2.1.1:
#   - Added explicit sys.path modification at the beginning of the script to ensure
#     the 'manager_utils' package (expected to be in the same directory as this script's
#     parent, i.e., the project root) can be found by Python's import mechanism.
#     This is a robust workaround if PYTHONPATH set by the calling Bash script isn't
#     being consistently effective for module resolution.
# --------------------------------------------------------------------------------
# (Original V2.1 documentation about SDE Roles, Component Pipeline, etc., follows)
# (For brevity, I'll assume the rest of the V2.1 doc block is here)
# --------------------------------------------------------------------------------

import sys # Must be imported first for sys.path manipulation
import os  # Must be imported for os.path

# --- SUREFIRE PATH MANIPULATION ---
# Calculate the project root directory (parent of the directory containing this script,
# OR the directory of this script if it's already at the root).
# This assumes coordinator.py is at the project root, and manager_utils is a subdirectory.
# If coordinator.py is in a subdir itself, adjust SCRIPT_DIR accordingly.
# For structure: $ROOT/coordinator.py and $ROOT/manager_utils/
SCRIPT_ABSOLUTE_PATH = os.path.abspath(sys.argv[0] if __file__ == sys.argv[0] else __file__)
PROJECT_ROOT_DIR = os.path.dirname(SCRIPT_ABSOLUTE_PATH)

# Add project root to Python's module search path
if PROJECT_ROOT_DIR not in sys.path:
    sys.path.insert(0, PROJECT_ROOT_DIR)
# --- END SUREFIRE PATH MANIPULATION ---

# Now, other imports should work if manager_utils is in PROJECT_ROOT_DIR
import json
import subprocess 
from typing import Dict, List, Any, Optional 

from manager_utils.logger_utils import logger
from manager_utils.process_runner import run_script
# from data_models import ... (if you had these)


# ROOT_DIR here is for _get_service_path, which might point to scripts
# within the same PROJECT_ROOT_DIR. It's consistent.
ROOT_DIR = PROJECT_ROOT_DIR # Use the calculated project root

def _get_service_path(env_var_name: str, default_script_name: str) -> str:
    path = os.environ.get(env_var_name)
    # If path is None and default_script_name is just a name, it will be joined with ROOT_DIR
    return path if path else os.path.join(ROOT_DIR, default_script_name)

# ... (rest of your coordinator.py V2.1 code remains IDENTICAL from here) ...
# --- Pipeline Status Constants ---
STATUS_MARKDOWN_TO_LATEX_CONVERSION_FAILED = "MARKDOWN_TO_LATEX_CONVERSION_FAILED"
STATUS_TRIVIAL_ERRORS_FOUND_REPORT_DIRECTLY = "TRIVIAL_ERRORS_FOUND_REPORT_DIRECTLY"
STATUS_MARKDOWN_LINT_PASSED_PROCEED_TO_LATEX_BUILD = "MARKDOWN_LINT_PASSED_PROCEED_TO_LATEX_BUILD"
STATUS_LEADS_PINPOINTED = "LEADS_PINPOINTED_FROM_LOG"

master_case_file: Dict[str, Any] = {}

def _stage_1_markdown_analysis(case_file: Dict[str, Any]) -> bool:
    logger.debug("Pipeline STAGE 1: Markdown Analysis", manager_name="Coordinator")
    ip_response = run_script(
        command_parts=["python3", _get_service_path("INITIAL_PROCESSOR_BIN", "initial_processor.py"), "--json-in", "--json-out"],
        input_json_obj={"case_id": case_file["case_id"], "markdown_content": case_file["original_document"]["content"]},
        expect_json_output=True, log_prefix_for_caller="Coordinator"
    )
    assert "tex_output" in ip_response and "status" in ip_response, "S1 Contract Violation: InitialProcessor response structure."
    
    case_file["latex_document_content"] = ip_response["tex_output"]["latex_document_content"] 
    case_file["md_to_latex_conversion_log"] = ip_response.get("tex_output", {}).get("conversion_log")
    case_file["initial_findings"] = ip_response.get("findings", []) 
    stage1_status = ip_response["status"]
    case_file["stage1_status"] = stage1_status

    if stage1_status == STATUS_MARKDOWN_TO_LATEX_CONVERSION_FAILED:
        logger.debug("S1 Result: MD->LaTeX conversion failed by InitialProcessor (user document issue).", manager_name="Coordinator")
        case_file["items_for_oracle_processing"] = [{
            "query_type": "MarkdownToLatexFailure", 
            "error_message": "Markdown to LaTeX Conversion Failed.",
            "details_for_oracle": case_file["md_to_latex_conversion_log"] or "Error during Markdown processing.",
            "original_markdown_content": case_file["original_document"]["content"] 
        }]
        case_file["final_report_trigger"] = "UserError_MarkdownConversion"
        return False 
    
    if stage1_status == STATUS_TRIVIAL_ERRORS_FOUND_REPORT_DIRECTLY:
        logger.debug("S1 Result: Trivial errors found by InitialProcessor. These are primary.", manager_name="Coordinator")
        case_file["items_for_oracle_processing"] = case_file.get("initial_findings", [])[:] 
        assert case_file["items_for_oracle_processing"], "Coordinator Logic Bug: TRIVIAL_ERRORS_FOUND but no findings."
        case_file["final_report_trigger"] = "UserError_MarkdownLint"
        return False 

    logger.debug(f"S1 Result: LaTeX generated. Lint status: {stage1_status}", manager_name="Coordinator")
    case_file.setdefault("items_for_oracle_processing", []).extend(case_file.get("initial_findings", []))
    return True

def _stage_2_tex_deep_analysis(case_file: Dict[str, Any]):
    logger.debug("Pipeline STAGE 2: TeX Deep Analysis", manager_name="Coordinator")
    assert case_file.get("latex_document_content"), "Coordinator Bug: LaTeX content missing for STAGE 2."

    comp_response = run_script(
        command_parts=["python3", _get_service_path("COMPILER_BIN", "compiler.py"), "--json-in", "--json-out"],
        input_json_obj={"case_id": case_file["case_id"], "tex_content": case_file["latex_document_content"]},
        expect_json_output=True, log_prefix_for_caller="Coordinator"
    )
    assert "compilation_output" in comp_response, "Compiler response contract: compilation_output missing."
    comp_output_data = comp_response["compilation_output"]
    assert "log_content" in comp_output_data, "Compiler response contract: log_content missing."
    case_file["latex_compilation_log"] = comp_output_data["log_content"]

    investigator_response = run_script(
        command_parts=["python3", _get_service_path("LOG_INVESTIGATOR_BIN", "log_investigator.py"), "--json-in", "--json-out"],
        input_json_obj={"case_id": case_file["case_id"],
                        "log_content": case_file["latex_compilation_log"],
                        "tex_content": case_file["latex_document_content"]},
        expect_json_output=True, log_prefix_for_caller="Coordinator"
    )
    assert "status" in investigator_response, "Investigator response contract: status missing."
    investigator_leads = investigator_response.get("leads", []) 
    assert investigator_leads, "Investigator contract violation: processed log but returned no leads."
    
    case_file.setdefault("items_for_oracle_processing", []).extend(investigator_leads)
    logger.debug(f"S2 Result: Investigator added {len(investigator_leads)} leads.", manager_name="Coordinator")

def _transform_item_to_oracle_query(item_data: Dict[str, Any], full_latex_content: Optional[str]) -> Dict[str, Any]:
    query = { "latex_content_full": full_latex_content } 
    if item_data.get("type") == "MarkdownToTexFailure":
        query["query_type"] = "MarkdownToLatexFailure"
        query["problem_description"] = item_data.get("error_message", "MD to TeX conversion failed.")
        query["error_signature_code"] = "MD_TO_TEX_FAILURE"
        query["document_context"] = {
            "problematic_snippet_original": item_data.get("details_for_oracle"),
            "original_markdown_content": item_data.get("original_markdown_content")
        }
    elif "finding_type" in item_data: 
        query["query_type"] = "InitialFinding"
        query["problem_description"] = item_data.get("message", "An initial finding was reported.")
        query["error_signature_code"] = item_data.get("type", "UNKNOWN_FINDING")
        query["document_context"] = {
            "problematic_line_in_latex": item_data.get("location_hint", {}).get("line", "unknown"),
            "problematic_snippet_latex": item_data.get("raw_details_for_oracle", {}).get("snippet")
        }
        query["original_markdown_location_hint"] = item_data.get("location_hint", {}).get("markdown_ref")
    elif "signature" in item_data: 
        query["query_type"] = "InvestigatedLead"
        query["problem_description"] = item_data.get("log_excerpt", "A LaTeX compilation error occurred.")
        query["error_signature_code"] = item_data.get("signature", "GENERIC_LATEX_ERROR")
        query["document_context"] = {
            "problematic_line_in_latex": item_data.get("line_in_tex", "unknown"),
            "problematic_snippet_latex": item_data.get("tex_context_snippet"),
            "relevant_log_excerpt": item_data.get("log_excerpt")
        }
    else:
        assert False, f"Coordinator Bug: Unknown item structure for Oracle query: {str(item_data)[:200]}"
    return query

def _stage_3_find_solutions(case_file: Dict[str, Any]):
    logger.debug("Pipeline STAGE 3: Find Solutions (Oracle)", manager_name="Coordinator")
    items_for_oracle = case_file.get("items_for_oracle_processing", [])
    assert items_for_oracle, "Coordinator Bug: Reached STAGE 3 with no items for Oracle."

    logger.debug(f"STAGE 3: Calling Oracle for {len(items_for_oracle)} item(s).", manager_name="Coordinator")
    
    generated_solutions = []
    for item_data in items_for_oracle:
        actual_oracle_request = {
            "case_id": case_file["case_id"],
            "item_to_solve": _transform_item_to_oracle_query(item_data, case_file.get("latex_document_content"))
            # Note: Oracle service's JSON input needs to match this structure for "item_to_solve"
        }
        solution_response = run_script(
            command_parts=["python3", _get_service_path("ORACLE_SOLUTION_GENERATOR_BIN", "oracle_solution_generator.py"), "--json-in", "--json-out"],
            input_json_obj=actual_oracle_request,
            expect_json_output=True, log_prefix_for_caller="Coordinator"
        )
        assert solution_response["status"] == "SolutionProposed", "Oracle contract violation (status)."
        assert "solution" in solution_response and solution_response["solution"], "Oracle contract (solution object)."
        generated_solutions.append(solution_response["solution"])
    
    case_file["collected_solutions"] = generated_solutions
    assert case_file["collected_solutions"], "Oracle processed items but no solutions collected (Oracle contract violation)."
    case_file["final_report_trigger"] = case_file.get("final_report_trigger", "UserError_SolutionsFound")

def _stage_4_report_to_user(case_file: Dict[str, Any]):
    logger.debug("Pipeline STAGE 4: Report to User (Reporter service call).", manager_name="Coordinator")
    assert case_file.get("final_report_trigger"), "Coordinator Bug: final_report_trigger not set."
    assert "collected_solutions" in case_file and case_file["collected_solutions"], \
        "Coordinator Bug: Reached reporting, but collected_solutions is empty."

    reporter_response = run_script(
        command_parts=["python3", _get_service_path("FINAL_REPORTER_BIN", "final_reporter.py"), "--json-in", "--json-out"],
        input_json_obj={
            "case_id": case_file["case_id"],
            "original_markdown": case_file["original_document"]["content"],
            "generated_tex": case_file.get("latex_document_content"),
            "overall_status": case_file["final_report_trigger"], 
            "solutions_and_findings": case_file["collected_solutions"]
        },
        expect_json_output=True, log_prefix_for_caller="Coordinator"
    )
    print(reporter_response["report_text"]) 
    sys.exit(1) 

def run_pipeline(initial_job_payload: dict):
    global master_case_file
    master_case_file = initial_job_payload 
    assert master_case_file.get("collected_solutions") == []
    assert master_case_file.get("latex_document_content") is None 
    assert master_case_file.get("items_for_oracle_processing") is None or \
           master_case_file.get("items_for_oracle_processing") == []
    master_case_file["final_report_trigger"] = "UserError_General" 

    logger.debug(f"Pipeline starting. Case: {master_case_file['case_id']}", manager_name="Coordinator")

    proceed_to_tex_analysis = _stage_1_markdown_analysis(master_case_file)
    if not proceed_to_tex_analysis: 
        _stage_3_find_solutions(master_case_file) 
        _stage_4_report_to_user(master_case_file) 
        assert False, "Coordinator Bug: Should have exited after Stage 1 reporting path."

    _stage_2_tex_deep_analysis(master_case_file)
    _stage_3_find_solutions(master_case_file) 
    _stage_4_report_to_user(master_case_file) 
    assert False, "Coordinator Bug: End of run_pipeline reached without exiting."


if __name__ == "__main__":
    logger.debug("Coordinator: Script started. Expecting InitialJobPayload JSON on stdin.", manager_name="Coordinator")
    initial_job_payload = json.loads(sys.stdin.read())
    
    assert "case_id" in initial_job_payload, "InitialJobPayload missing 'case_id'"
    assert "original_document" in initial_job_payload and \
           "content" in initial_job_payload.get("original_document", {}), \
           "InitialJobPayload 'original_document.content' missing"
    assert initial_job_payload.get("collected_solutions") == [], "Payload contract: 'collected_solutions: []'"
    assert initial_job_payload.get("latex_document_content") is None, "Payload contract: 'latex_document_content: null'"
    assert initial_job_payload.get("items_for_oracle_processing") is None or \
           initial_job_payload.get("items_for_oracle_processing") == [], \
           "Payload contract: 'items_for_oracle_processing: []' or null"

    run_pipeline(initial_job_payload)
